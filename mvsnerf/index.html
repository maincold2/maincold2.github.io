<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo">
    <meta name="author" content="Anpei Chen*,
                                Zexiang Xu*,
                                Fuqiang Zhao,
                                Xiaoshuai Zhang,
								Fanbo Xiang,
                                Jingyi Yu,
                                Hao Su">

    <title>MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>MVSNeRF: Fast Generalizable Radiance Field Reconstruction <br> from Multi-View Stereo</h2>
        <h3>ICCV 2021</h3>
           <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>
    <hr>
    <p class="authors">
        <a href="https://apchenstu.github.io/"> Anpei Chen*</a>,
        <a href="http://cseweb.ucsd.edu/~zex014/"> Zexiang Xu*</a>,
        <a > Fuqiang Zhao</a>,
        <a href="https://i.buriedjet.com/"> Xiaoshuai Zhang</a>,
        <a href="https://www.fbxiang.com/"> Fanbo Xiang</a> </br>
        <a href="https://sist.shanghaitech.edu.cn/2020/0707/c7499a53862/page.htm"> Jingyi Yu</a>,
        <a href="https://cseweb.ucsd.edu/~haosu/"> Hao Su</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2103.15595">Paper</a>
        <a class="btn btn-primary" href="https://github.com/apchenstu/mvsnerf">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/3M3edNiaGsA" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis.
            Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images,
            we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference.
            Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction.
            We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability.
            Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction.
            Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF. 
        </p>
    </div>


    <div class="section">
        <h2>Pipeline</h2>
        <hr>
        <p>
            Our framework first constructs a <b>cost volume</b> (a) by warping 2D image features onto a plane sweep. We then apply 3D CNN to reconstruct a <b>neural encoding volume</b> with per-voxel neural features (b). We use an MLP to regress volume density and RGB radiance at an arbitrary location using features interpolated from the encoding volume. These volume properties are used by differentiable ray marching for final rendering (c).
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Result</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
        <p>
            We train our MVSNeRF with scenes of objects in the DTU dataset. Our network can effectively <b>generalize</b> across diverse scenes; even for a complex indoor scene, our network can reconstruct a neural radiance field from only three input images (a) and synthesize a realistic image from a novel viewpoint (b). While this result contains artifacts, it can be largely improved by fine-tuning our reconstruction on more images for only <b>6 min</b> (c), which achieves better quality than the NeRF's nerf result (d) from 9.5h per-scene optimization. 
        </p>
    </div>

    <div class="section">
        <h2>Results</h2>
        <hr>

        <div class="col justify-content-center text-center">
            <img src="img/progress.png" style="width:100%; margin-right:-10px; margin-top:10px;">

        <p>
            Optimization progress. We show results of our fine-tuning (top) and optimizing a <b>NeRF</b> (bottom) with different time periods. Our <b>0-min</b> result refers to the initial output from our network inference. Note that our <b>18-min</b> results are already much better than the <b>215-min</b> NeRF results. PSNRs of the image crops are shown in the figure.
        </p>

        <div class="col justify-content-center text-center">
            <img src="img/result.png" style="width:100%; margin-right:-10px; margin-top:10px;">

        <p>
            Rendering quality comparison. On the left, we show rendering results of our method and concurrent neural rendering methods PixelNeRF, IBRNet by directly running the networks. We show our 15-min fine-tuning results and NeRF's  10.2h-optimization results on the right.
        </p>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2103.15595"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @misc{chen2021mvsnerf,
                  title={MVSNeRF: Fast Generalizable Radiance Field Reconstruction 
                        from Multi-View Stereo}, 
                  author={Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang 
                          and Fanbo Xiang and Jingyi Yu and Hao Su},
                  year={2021},
                  eprint={2103.15595},
                  archivePrefix={arXiv},
                  primaryClass={cs.CV}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="https://apchenstu.github.io/">Anpei Chen</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
