<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Joo Chan Lee,
                                Daniel Rho,
                                Jong Hwan Ko,
                                Eunbyung Park">

    <title>FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos</title>
    
    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="title">
    <div class="container"></div>
    <s2>FFNeRV</s2>
   <h3 class="nerf_subheader_v2">Flow-Guided Frame-Wise Neural Representations for Videos</h2>
        <!-- <h3 class="nerf_subheader_v2">ECCV 2022 (scores: 1, 1, 1)</h3> -->
           <!-- <p class="abstract">A compact and efficent scene representation</p> -->
    <hr>
    <p class="authors">
        <!-- <a href="https://apchenstu.github.io/"> Anpei Chen*</a>, -->
        <a href="https://maincold2.github.io/"> Joo Chan Lee</a>,
        <a> Daniel Rho</a>,
        <a href="https://iris.skku.edu"> Jong Hwan Ko<sup>†</sup></a>,
        <a href="https://silverbottlep.github.io/"> Eunbyung Park<sup>†</sup></a>

    </p>
    <p class="authors">
        Sungkyunkwan University
    </p>
    
    
    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star"></span><sup>†</sup> Corresponding Authors</div>
    
    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn" href="">[Paper]</a>
        <a class="btn" href="">[Code]</a>
        <!-- <a class="btn btn-primary" href="review.pdf">Review</a>
        <a class="btn btn-primary" href="rebuttal.pdf">Rebuttal</a>
        <a class="btn btn-primary" href="meta-review.pdf">Meta-review</a> -->
    </div>
</div>




    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section">
        <div class="container">
            <s3>Overview</s3>
            <hr>
            <video  width=100% autoplay muted controls loop preload="metadata">
                          <source src="video/ready_demo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
            </video>
<!--             <div class="w-slide">
                <div class="div-block-9">
                    <div class="video_class w-embed">
                        <video  width=100% autoplay muted controls loop preload="metadata">
                          <source src="video/ready_demo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="video_class mobile w-embed">
                        <video  width=100% muted controls loop preload="metadata">
                          <source src="video/ready_demo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
            </div> -->
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                We propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to remove the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, improving the continuity of spatial features.
            </p>
            <div class="columns-5 w-row">
                <img src="img/fig_abs.png" style="width:50%; margin-right:0px; margin-top:0px;">
            </div>
<!--            <img src="img/teaser_v6.png" alt="" class="nerf_network"/>-->
            <p class="paragraph-3 nerf_text">
                Experimental results show that FFNeRV yields the best performance for video compression and frame interpolation among the methods using frame-wise representations or neural fields. To reduce the model size even further, we devise a more compact convolutional architecture using the group and pointwise convolutions. With all other model compression techniques, including quantization-aware training and entropy coding, FFNeRV outperforms widely-used standard video codecs (H.264 and HEVC) and performs on par with state-of-the-art video compression algorithms.
            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Inspired by the standard video codecs, we incorporate optical flows into the frame-wise representation to remove the temporal redundancy.
                FFNeRV generates a video frame by aggregating neighboring frames guided by flows, enforcing the reuse of pixels from other frames.
                It encourages the network not to waste parameters by memorizing the same pixel values across frames, greatly enhancing parameter efficiency.
            </p>
            <p class="paragraph-3 nerf_text">
                Motivated by the grid-based neural representations, we propose to use multi-resolution temporal grids with a fixed spatial resolution to map continuous temporal coordinates to corresponding latent features to further improve the compression performance.
                We also propose using a more compact convolutional architecture.
                We employ group and pointwise convolutions in the suggested frame-wise flow representations, motivated by lightweight neural networks and generative models producing high-quality images.
            </p>
            <div class="columns-5 w-row">
                <img src="img/fig_arch.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>


            <!-- <div class="columns-5 w-row">
                <img src="img/fig_agg.png" style="width:50%; margin-right:0px; margin-top:10px;">
            </div> -->
    
            <!-- <p class="paragraph-3 nerf_text">
                We now present our TensoRF representation and reconstruction. For each shading location <b>x</b> = (x,y,z), we use linearly/bilinearly sampled values from the vector (<b>v</b>)/matrix (<b>M</b>) factors to compute the corresponding trilinearly interpolated values of the tensor components.
                The density component values (A<sub>σ</sub>(x)) are summed to get the volume density directly (σ).
                The appearance values (A<sub>c</sub>(x)) are concatenated into a vector (⊕[A<sub>c</sub><sup style="margin-left:-8px">m</sup>(x)]<sub>m</sub>) that is then multiplied by an appearance matrix (<b>B</b>) and sent to the decoding function S for RGB color (c) regression.
                The decoding function S can be a Spherical Harmonic (SH) function or a fully-connected network (FCN).
            </p> -->
        </div>
    </div>



    <!-- </br></br> -->
    <div class="section">
        <div class="container">
        <s3>Performance</s3>
        <hr>
        <h2 class="grey-heading_nerf">
                Video Representation
        </h2>

        <p class="paragraph-3 nerf_text">
            The square area (a) in the following figure is the background and appears constantly in the neighboring frames.
            Although the independent frame misses this faint part, the aggregated frame captures it by referencing neighboring independent frames.
            The network adds these missing fine details to the independent frame by retracting those details from nearby frames, as shown in the weight map, in order to improve the quality of the final output.
            On the other hand, part (b) contains a rapidly moving object, so that the edge of it is blurred when aggregating multiple frames.
            Hence, the weight pixels corresponding to the part are large for the independent frame.
            Through the compatibility of the independent and aggregated frames, our approach assures high performance.
        </p>
        <div class="columns-5 w-row">
            <img src="img/fig_rep1.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
        <p class="paragraph-3 nerf_text">
            FFNeRV outperforms other frame-wise methods for video representation.
        </p>
        <div class="columns-5 w-row">
            <img src="img/fig_rep2.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
        <!-- <div class="w-slide">
              <div class="div-block-9">
                  <div class="video_class w-embed">
                      <video  width=100% height=100% autoplay muted controls loop preload="metadata" poster="video/poster/train_process.jpg">
                        <source src="video/ready_demo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </div>
                  <div class="video_class mobile w-embed">
                      <video  width=100% height=100% muted controls loop preload="metadata" poster="video/poster/train_process.jpg">
                        <source src="video/ready_demo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </div>
              </div>
          </div> training process -->
    
        <!-- <p class="paragraph-3 nerf_text">
            Note that, unlike concurrent works <a href="https://alexyu.net/plenoxels/"> Plenoxels</a>
            and  <a href="https://nvlabs.github.io/instant-ngp/"> Instant-ngp</a>
            that require customized CUDA kernels, our model’s
            efficiency gains are obtained using a standard PyTorch implementation.
        </p> -->
    
        <h2 class="grey-heading_nerf">
            Video Frame Interpolation
        </h2>
        <p class="paragraph-3 nerf_text">
            FFNeRV outperforms other frame-wise methods for video frame interpolation.
        </p>
    
        <div class="columns-5 w-row">
            <img src="img/fig_intp1.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
        <div class="columns-5 w-row">
            <img src="img/fig_intp2.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
    
        <!-- <p class="paragraph-3 nerf_text">
            The above figure shows the checkpoint size on Synthetic-NeRF dataset (without compression), less always is more.
        </p> -->
    
        <h2 class="grey-heading_nerf">
            Video Compression
        </h2>
        <p class="paragraph-3 nerf_text">
            We present the effectiveness of FFNeRV for a major video task, video compression, comparing with state-of-the-arts methods on the UVG dataset.
            Even without pruning, FFNeRV outperforms widely-used standard video codecs and neural video representations and performs on par with state-of-the-art video compression algorithms.
        </p>
        <div class="columns-5 w-row">
            <img src="img/fig_comp.png" style="width:50%; margin-right:0px; margin-top:0px;">
        </div>

<!--        <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></div>-->
        
<!--     
            <div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div>
                    <div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div>
                    <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div>
            </div>
        </div>
        </div> -->


        

<!--        <h2 class="grey-heading_nerf">-->
<!--            Extension-->
<!--        </h2>-->
<!--        <p class="paragraph-3 nerf_text">-->
<!--            As far as we know, our work is the first that views radiance field modeling from a-->
<!--            tensorial perspective and pose the problem of radiance field reconstruction as-->
<!--            one of low-rank tensor reconstructions.-->
<!--        </p>-->

        <!-- </br></br></br>
        <s2> Concurrent works </s2>
        <hr> -->
        <!-- <div class="section"> -->
        <!-- <p class="paragraph-3 nerf_text">
        Some concurrent works also focus on speeding up the training process or compact representation:</p> -->
        <!-- <div class="container"> -->
         <!-- <ul>
            <li style="text-align: left">Dense grid with a shallow MLP. <a href="https://sunset1995.github.io/dvgo/"> DVGO </a> </li>
            <li style="text-align: left">Sparse grid — Plenoxels: Radiance Fields without Neural Networks. <a href="https://alexyu.net/plenoxels/"> Plenoxels </a> </li>
            <li style="text-align: left">Hash — Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. <a href="https://nvlabs.github.io/instant-ngp/"> iNGP </a> </li>
         </ul> -->
        <!-- </ul> -->
    <!-- </div> -->




        <!-- </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
            Qiangeng Xu for providing some baseline results,
            and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
            This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
        </p> -->
<!--    </div>-->

<!--        </br></br>-->
<!--        <s2>Arxiv</s2>-->
<!--        <hr>-->
<!--        <div>-->
<!--            <div class="list-group">-->
<!--                <a href="https://arxiv.org/abs/2103.15595"-->
<!--                   class="list-group-item">-->
<!--                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">-->
<!--                </a>-->
<!--            </div>-->
<!--        </div>-->

        <!-- </br></br> -->
   <div class="section">
    <div class="container"> 
        <s3>Bibtex</s3>
        <hr>
        <div class="bibtexsection">
            @article{lee2022ffnerv,
                title={FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos},
                author={Lee, Joo Chan and Rho, Daniel and Ko, Jong Hwan and Park, Eunbyung},
                journal={arXiv preprint arXiv:},
                year={2022}
            }
        </div>
   </div>
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from <a href="https://apchenstu.github.io/TensoRF/">TensoRF</a>.
    </footer>
</div>

<!-- 
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script> -->

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
