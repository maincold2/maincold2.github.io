<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Joo Chan Lee,
                                Daniel Rho,
                                Jong Hwan Ko,
                                Eunbyung Park">

    <title>FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos</title>
    
    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="title">
    <div class="container"></div>
    <s2>FFNeRV</s2>
   <h3 class="nerf_subheader_v2">Flow-Guided Frame-Wise Neural Representations for Videos</h3>
   <h4>ACM Multimedia 2023 (MM '23)</h4>
    <hr>
    <p class="authors">
        <a href="https://maincold2.github.io/"> Joo Chan Lee</a>,
        <a href="https://daniel03c1.github.io/"> Daniel Rho</a>,
        <a href="https://iris.skku.edu/"> Jong Hwan Ko<sup>†</sup></a>,
        <a href="https://silverbottlep.github.io/"> Eunbyung Park<sup>†</sup></a>

    </p>
    <p class="authors">
        Sungkyunkwan University
    </p>
    
    
    <div class="nerf_equal_v2"><span class="text-span_nerf_star"></span><sup>†</sup> Corresponding Authors</div>
    
    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn" href="https://arxiv.org/abs/2212.12294">[Paper]</a>
        <a class="btn" href="https://github.com/maincold2/FFNeRV">[Code]</a>
    
    </div>
</div>




    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section">
        <div class="container">
            <s3>Overview</s3>
            <hr>
            <video  width=100% autoplay muted controls loop preload="metadata">
                <source src="video/ready_demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <img src="img/fig_abs.png" style="width:45%; margin-left: 10px; margin-right:25px; margin-top:0px; float:right">
            <p class="paragraph-3 nerf_text">
                Neural representations have shown a remarkable capability of representing, generating, and manipulating various forms of signals. For videos, however, mapping pixel-wise coordinates to their quantities has shown relatively low compression performance and slow convergence and inference speed <b>(a)</b>.
                Frame-wise video representation, which maps a temporal coordinate to its entire frame <b>(b)</b>, has recently emerged as an alternative method to represent videos, improving compression rates and encoding speed. While promising, it has still failed to reach the performance of state-of-the-art video compression algorithms.
                In this work, we propose FFNeRV <b>(c)</b>, a novel method for incorporating flow information into frame-wise representations to exploit the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, improving the continuity of spatial features.
            </p>
            <p class="paragraph-3 nerf_text">
                Experimental results show that FFNeRV yields the best performance for video compression and frame interpolation among the methods using frame-wise representations or neural fields. To reduce the model size even further, we devise a more compact convolutional architecture using the group and pointwise convolutions. With all other model compression techniques, including quantization-aware training and entropy coding, FFNeRV outperforms widely-used standard video codecs (H.264 and HEVC) and performs on par with state-of-the-art video compression algorithms.
            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Inspired by the standard video codecs, we incorporate optical flows into the frame-wise representation to remove the temporal redundancy.
                FFNeRV generates a video frame by aggregating neighboring frames guided by flows, enforcing the reuse of pixels from other frames.
                It encourages the network not to waste parameters by memorizing the same pixel values across frames, greatly enhancing parameter efficiency.
            </p>
            <p class="paragraph-3 nerf_text">
                Motivated by the grid-based neural representations, we propose to use multi-resolution temporal grids with a fixed spatial resolution to map continuous temporal coordinates to corresponding latent features to further improve the compression performance.
                We also propose using a more compact convolutional architecture.
                We employ group and pointwise convolutions in the suggested frame-wise flow representations, motivated by lightweight neural networks and generative models producing high-quality images.
            </p>
            <div class="columns-5 w-row">
                <img src="img/fig_arch.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>

            <h2 class="grey-heading_nerf">
                What makes it effective
            </h2>
            <p class="paragraph-3 nerf_text">
                The square area (a) in the following figure is the background and appears constantly in the neighboring frames.
                Although the independent frame misses this faint part, the aggregated frame captures it by referencing neighboring independent frames.
                The network adds these missing fine details to the independent frame by retracting those details from nearby frames, as shown in the weight map, in order to improve the quality of the final output.
                On the other hand, part (b) contains a rapidly moving object, so that the edge of it is blurred when aggregating multiple frames.
                Hence, the weight pixels corresponding to the part are large for the independent frame.
                Through the compatibility of the independent and aggregated frames, our approach assures high performance.
            </p>
            <div class="columns-5 w-row">
                <figure>
                <img src="img/fig_rep1.png" style="width:90%; margin-right:0px; margin-top:0px;">
                <!-- <figcaption><i>I(t)</i></figcaption> -->
                </figure>
            </div>
        </div>
    </div>



    <!-- </br></br> -->
    <div class="section">
        <div class="container">
        <s3>Performance</s3>
        <hr>
        <h2 class="grey-heading_nerf">
            Video Compression
        </h2>
        <img src="img/fig_comp.png" style="width:70%; margin-left: 0px; margin-right:0px; margin-top:0px;">
        <p class="paragraph-3 nerf_text">
        We present the effectiveness of FFNeRV for a major video task, video compression, comparing with state-of-the-art methods on the UVG dataset.
        Even without pruning, FFNeRV outperforms widely-used standard video codecs and neural video representations and performs on par with state-of-the-art video compression algorithms.
        </p>
        <br><br><br>
        <h2 class="grey-heading_nerf">
                Video Representation
        </h2>


        <p class="paragraph-3 nerf_text">
            FFNeRV outperforms other frame-wise methods for video representation.
        </p>
        <div class="columns-5 w-row">
            <img src="img/fig_rep2.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
    
        <h2 class="grey-heading_nerf">
            Video Frame Interpolation
        </h2>
        <p class="paragraph-3 nerf_text">
            FFNeRV outperforms other frame-wise methods for video frame interpolation.
        </p>
    
        <div class="columns-5 w-row">
            <img src="img/fig_intp1.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
        <div class="columns-5 w-row">
            <img src="img/fig_intp2.png" style="width:90%; margin-right:0px; margin-top:0px;">
        </div>
    



        <!-- </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
            Qiangeng Xu for providing some baseline results,
            and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
            This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
        </p> -->

   <div class="section">
    <div class="container"> 
        <s3>Bibtex</s3>
        <hr>
        <div class="bibtexsection">
            @inproceedings{lee2023ffnerv,
                title={FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos},
                author={Lee, Joo Chan and Rho, Daniel and Ko, Jong Hwan and Park, Eunbyung},
                booktitle = {Proceedings of the ACM International Conference on Multimedia},
                year={2023},
                pages = {7859–7870},
            }

        </div>
   </div>
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from <a href="https://apchenstu.github.io/TensoRF/">TensoRF</a>.
    </footer>
</div>

</body>
</html>
