<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Joo Chan Lee,
                                Daniel Rho,
                                Jong Hwan Ko,
                                Eunbyung Park">

    <title>FFNeRV: Flow-guided Frame-wise Neural Representations for Videos</title>
    
    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">FFNeRV</h1>
   <h2 class="nerf_subheader_v2">Flow-Guided Frame-Wise Neural Representations for Videos</h2>
        <!-- <h3 class="nerf_subheader_v2">ECCV 2022 (scores: 1, 1, 1)</h3> -->
           <!-- <p class="abstract">A compact and efficent scene representation</p> -->
    <hr>
    <p class="authors">
        <!-- <a href="https://apchenstu.github.io/"> Anpei Chen*</a>, -->
        <a href="https://maincold2.github.io/"> Joo Chan Lee</a>,
        <a> Daniel Rho</a>,
        <a href="https://iris.skku.edu"> Jong Hwan Ko*</a>,
        <a href="https://silverbottlep.github.io/"> Eunbyung Park*</a>

    </p>
    
    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Corresponding Authors</div>
    
    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn" href="">Paper</a>
        <a class="btn" href="">Code</a>
        <!-- <a class="btn btn-primary" href="review.pdf">Review</a>
        <a class="btn btn-primary" href="rebuttal.pdf">Rebuttal</a>
        <a class="btn btn-primary" href="meta-review.pdf">Meta-review</a> -->
    </div>
</div>




    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                We propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to remove the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, improving the continuity of spatial features.
            </p>
            <div class="columns-5 w-row">
                <img src="img/fig_abs.png" style="width:50%; margin-right:0px; margin-top:0px;">
            </div>
<!--            <img src="img/teaser_v6.png" alt="" class="nerf_network"/>-->
            <p class="paragraph-3 nerf_text">
                Experimental results show that FFNeRV yields the best performance for video compression and frame interpolation among the methods using frame-wise representations or neural fields. To reduce the model size even further, we devise a more compact convolutional architecture using the group and pointwise convolutions. With all other model compression techniques, including quantization-aware training and entropy coding, FFNeRV outperforms widely-used standard video codecs (H.264 and HEVC) and performs on par with state-of-the-art video compression algorithms.
            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Inspired by the standard video codecs, we incorporate optical flows into the frame-wise representation to remove the temporal redundancy.
                FFNeRV generates a video frame by aggregating neighboring frames guided by flows, enforcing the reuse of pixels from other frames.
                It encourages the network not to waste parameters by memorizing the same pixel values across frames, greatly enhancing parameter efficiency.
            </p>
            <p class="paragraph-3 nerf_text">
                Motivated by the grid-based neural representations, we propose to use multi-resolution temporal grids with a fixed spatial resolution to map continuous temporal coordinates to corresponding latent features to further improve the compression performance.
                We also propose using a more compact convolutional architecture.
                We employ group and pointwise convolutions in the suggested frame-wise flow representations, motivated by lightweight neural networks and generative models producing high-quality images.
            </p>
            <div class="columns-5 w-row">
                <img src="img/fig_arch.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>


            <!-- <div class="columns-5 w-row">
                <img src="img/fig_agg.png" style="width:50%; margin-right:0px; margin-top:10px;">
            </div> -->
    
            <!-- <p class="paragraph-3 nerf_text">
                We now present our TensoRF representation and reconstruction. For each shading location <b>x</b> = (x,y,z), we use linearly/bilinearly sampled values from the vector (<b>v</b>)/matrix (<b>M</b>) factors to compute the corresponding trilinearly interpolated values of the tensor components.
                The density component values (A<sub>σ</sub>(x)) are summed to get the volume density directly (σ).
                The appearance values (A<sub>c</sub>(x)) are concatenated into a vector (⊕[A<sub>c</sub><sup style="margin-left:-8px">m</sup>(x)]<sub>m</sub>) that is then multiplied by an appearance matrix (<b>B</b>) and sent to the decoding function S for RGB color (c) regression.
                The decoding function S can be a Spherical Harmonic (SH) function or a fully-connected network (FCN).
            </p> -->
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
        <hr>
        <h2 class="grey-heading_nerf">
                Video Representation
        </h2>
        <!-- <p class="paragraph-3 nerf_text">
            Given a set of multi-view input images with known camera poses,
            our tensorial radiance field is optimized per scene via gradient descent,
            minimizing an L2 rendering loss, using only the ground truth pixel colors as supervision.
        </p> -->
        <div class="w-slide">
              <div class="div-block-9">
                  <div class="video_class w-embed">
                      <video  width=100% height=100% autoplay muted controls loop preload="metadata" poster="video/poster/train_process.jpg">
                        <source src="video/ready_demo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </div>
                  <div class="video_class mobile w-embed">
                      <video  width=100% height=100% muted controls loop preload="metadata" poster="video/poster/train_process.jpg">
                        <source src="video/ready_demo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </div>
              </div>
          </div><!--  training process -->
    
        <!-- <p class="paragraph-3 nerf_text">
            Note that, unlike concurrent works <a href="https://alexyu.net/plenoxels/"> Plenoxels</a>
            and  <a href="https://nvlabs.github.io/instant-ngp/"> Instant-ngp</a>
            that require customized CUDA kernels, our model’s
            efficiency gains are obtained using a standard PyTorch implementation.
        </p> -->
    
        <h2 class="grey-heading_nerf">
            Video Frame Interpolation
        </h2>
        <!-- <p class="paragraph-3 nerf_text">
            In contrast to previous works that directly reconstruct voxels,
            our tensor factorization reduces space complexity from O(n<sup>3</sup>) to O(n) (with CP) or O(n<sup>2</sup>) (with VM),
            significantly lowering memory footprint.
        </p> -->
    
        <div class="columns-5 w-row">
            <img src="img/fig_intp1.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>
        <div class="columns-5 w-row">
            <img src="img/fig_intp2.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>
    
        <!-- <p class="paragraph-3 nerf_text">
            The above figure shows the checkpoint size on Synthetic-NeRF dataset (without compression), less always is more.
        </p> -->
    
        <h2 class="grey-heading_nerf">
            Video Compression
        </h2>
        <div class="columns-5 w-row">
            <img src="img/fig_comp.png" style="width:50%; margin-right:0px; margin-top:0px;">
        </div>
        <!-- <p class="paragraph-3 nerf_text">
             Our approach can also achieve high-quality radiance field reconstruction
             for 360<sup>o</sup> objects and forward-facing scenes.
             All results without compression are available at <a href="https://1drv.ms/u/s!Ard0t_p4QWIMgQ2qSEAs7MUk8hVw?e=lwKlme/"> OneDrive</a>.
        </p> -->

<!--        <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></div>-->
        
    
            <div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div>
                    <div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div>
                    <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div>
            </div>
        </div>


        

<!--        <h2 class="grey-heading_nerf">-->
<!--            Extension-->
<!--        </h2>-->
<!--        <p class="paragraph-3 nerf_text">-->
<!--            As far as we know, our work is the first that views radiance field modeling from a-->
<!--            tensorial perspective and pose the problem of radiance field reconstruction as-->
<!--            one of low-rank tensor reconstructions.-->
<!--        </p>-->

        <!-- </br></br></br>
        <s2> Concurrent works </s2>
        <hr> -->
        <!-- <div class="section"> -->
        <!-- <p class="paragraph-3 nerf_text">
        Some concurrent works also focus on speeding up the training process or compact representation:</p> -->
        <!-- <div class="container"> -->
         <!-- <ul>
            <li style="text-align: left">Dense grid with a shallow MLP. <a href="https://sunset1995.github.io/dvgo/"> DVGO </a> </li>
            <li style="text-align: left">Sparse grid — Plenoxels: Radiance Fields without Neural Networks. <a href="https://alexyu.net/plenoxels/"> Plenoxels </a> </li>
            <li style="text-align: left">Hash — Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. <a href="https://nvlabs.github.io/instant-ngp/"> iNGP </a> </li>
         </ul> -->
        <!-- </ul> -->
    <!-- </div> -->




        <!-- </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
            Qiangeng Xu for providing some baseline results,
            and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
            This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
        </p> -->
<!--    </div>-->

<!--        </br></br>-->
<!--        <s2>Arxiv</s2>-->
<!--        <hr>-->
<!--        <div>-->
<!--            <div class="list-group">-->
<!--                <a href="https://arxiv.org/abs/2103.15595"-->
<!--                   class="list-group-item">-->
<!--                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">-->
<!--                </a>-->
<!--            </div>-->
<!--        </div>-->

        <!-- </br></br> -->
<!--    <div class="section">-->
        <!-- <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @INPROCEEDINGS{Chen2022ECCV,
              author = {Anpei Chen and Zexiang Xu and Andreas Geiger and Jingyi Yu and Hao Su},
              title = {TensoRF: Tensorial Radiance Fields},
              booktitle = {European Conference on Computer Vision (ECCV)},
              year = {2022}
            }
        </div> -->
<!--    </div>-->
    <!-- </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
            Send feedback and questions to <a href="https://apchenstu.github.io/">Anpei Chen</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script> -->

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>

