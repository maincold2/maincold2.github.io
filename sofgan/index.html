<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Deep Surface Light Fields">
    <meta name="author" content="Anpei Chen*,
                                Ruiyang Liu*,
                                Ling Xie,
                                Zhang Chen,
								Hao Su,
                                Jingyi Yu">

    <title>SofGAN: A Portrait Image Generator with Dynamic Styling</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>SofGAN: A Portrait Image Generator with Dynamic Styling</h2>
     <h3>ACM Transactions on Graphics 2021</h3>
           <p class="abstract">A GAN-based image generator with explicit attribute controlling.</p>
    <hr>
    <p class="authors">
        <a href="https://apchenstu.github.io/"> Anpei Chen*</a>,
        <a > Ruiyang Liu*</a>,
        <a href="https://jack12xl.netlify.app/"> Ling Xie</a>,
        <a href="https://scholar.google.com/citations?user=4MIbSrAAAAAJ&hl=en"> Zhang Chen</a></br>
        <a href="https://cseweb.ucsd.edu/~haosu/"> Hao Su</a>,
        <a href="http://www.yu-jingyi.com/cv/"> Jingyi Yu</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2007.03780">Paper</a>
        <a class="btn btn-primary" href="https://github.com/apchenstu/sofgan">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/xig8ZA3DVZ8" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            Recently, GANs have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a <b>SofGAN</b> image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a <b>semantic occupancy field (SOF)</b>. The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texture maps and stylized to a portrait photo using our <b>semantic instance-wise (SIW)</b> module.
            Through extensive experiments, we show that our system can generate high quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications such as appearance-consistent facial animation and dynamic styling. 
        </p>
    </div>


    <div class="section">
        <h2>Overview</h2>
        <hr>
        <p>
            First row: our decoupled representation allows explicit control over pose, shape and texture styles. Starting from the source image, we explicitly change it's head pose (2nd image), facial/hair contour (3rd image) and texture styles. Second row: interactive image generation from incomplete segmaps. We allow users to gradually add parts to the segmap and generate colorful images on-the-fly.
        </p>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/semantic_level.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Applications</h2>
        <hr>
        <p align="center" >
            The first two videos demonstrate the regional style adjustment conditing on specify semantic</br>
            segmentation map. One of the key features of our SIW-StyleGAN is semantic-level style controlling.</br> 
            Benefiting from the StyleConv blocks and style mixing training strategy, we could separately</br> 
            control the style for each semantic region.
        </p>


        <div class="vcontainer">
            <video src="img/regional_style.mp4" poster="img/regional_style.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 1:</b> Regional style adjustment
        </p>
        <br>

        <div class="vcontainer">
            <video src="img/style_transfer.mp4" poster="img/style_transfer.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 2:</b> Style transfer
        </p>
        <br>

        <p align="center" >
            Under our generation framework, we can generate free-viewpoint portrait images</br>
            from geometric samples or real images by changing the camera pose. our SOF is</br>
            trained with multi-view semantic segmentation maps, the geometric projection</br> 
            constraint between views is encoded in the SOF, which enables our method to</br> 
            keep shape and expression consistent when changing the viewpoint.
        </p>

        <div class="vcontainer">
            <video src="img/fvv.mp4" poster="img/fvv.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 3a:</b> Free view point rendering and shape morphing
        </p>
        <br>

        <div class="vcontainer">
            <video src="img/fvv2.mp4" poster="img/fvv2.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 3b:</b> Free view point rendering and shape morphing
        <br>

        <p align="center" >
            we collect a video clip from Internet and generate segmentation maps for each</br>
            frame with a pre-trained face parser. Our method can preserve texture style and</br>
            shape consistency on various poses and expressions without any temporal regularization.
        </p>

        <div class="vcontainer">
            <video src="img/animation.mp4" poster="img/animation.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 4:</b> Animated video sequences
        </p>
        <br>  

        <p align="center" >
            Our method allow users to gradually add parts to the segmap and generate colorful images on-the-fly.
        </p>

        <div class="vcontainer">
            <video src="img/drawing.mp4" poster="img/drawing.png"  width="960" height="540" controls preload></video>
        </div>
        <p align="center">
            <b>Video 5a:</b> Generation from drawing
        </p>
        <br>  

        <div align="center" class="vcontainer">
            <video src="img/drawing2.mp4" width="720" height="544" controls preload></video>
        </div>
        <p align="center">
            <b>Video 5b:</b> An awesome online drawing app <a href="https://apps.apple.com/cn/app/wand/id1574341319"> Wand</a>,  made by  <a href="https://www.deemos.com//"> 影眸科技</a>. 
        </p>

    </div>

    <div class="section">
        <h2>Acknowledgments</h2>
        <hr>
        <p>
            We thank Xinwei Li and Qiuyue Wang for dubbing the video, Zhixin Piao for comments and discussions, and Kim Seonghyeon
            and Adam Geitgey for sharing their StyleGAN2 implementation and face recognition code for our comparisons and quantity evaluation.
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @article{sofgan,
            author = {Chen, Anpei and Liu, Ruiyang and Xie, Ling and Chen, Zhang and 
            Su, Hao and Yu Jingyi},
            title = {SofGAN: A Portrait Image Generator with Dynamic Styling},
            year = {2021},
            issue_date = {July 2021},
            publisher = {Association for Computing Machinery},
            address = {New York, NY, USA},
            volume = {41},
            number = {1},
            url = {https://doi.org/10.1145/3470848},
            doi = {10.1145/3470848},
            journal = {ACM Trans. Graph.},
            month = July,
            articleno = {1},
            numpages = {26},
            keywords = {Image editing, Generative adversarial networks}
            }
        </div>
    </div>

    <hr>



    <footer>
        <p>Send feedback and questions to <a href="https://apchenstu.github.io/">Anpei Chen</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
